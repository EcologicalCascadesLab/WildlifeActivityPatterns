---
title: "Pigs and Macaques Multinomial Model Selection"
author: "Samuel Xin Tham Lee"
date: "2023-10-03"
output: html_document
---

Here, we will carry out model selection to observe the effects of forest integrity on the diel activity on pigs and macaques.

```{r setup, include=FALSE}
## start with clean enviro
rm(list = ls())

### Set Working directories
## Markdown requires ABSOLUTE paths, not relative paths. 
# Adjust the code accordingly for your machine. 

# set WD --> must be run in console, not code chunk! 
setwd("C:/Users/samle/Dropbox/Sam Lee Honours/SEA Activity Data Analysis Sam Honours/SEA Activity Temporal Shift Analysis Sam Honours/SEA Activity temporal shift by community [Multinomial] Sam Honours") 
# and knit to the same place. 
knitr::opts_knit$set(root.dir = "C:/Users/samle/Dropbox/Sam Lee Honours/SEA Activity Data Analysis Sam Honours/SEA Activity Temporal Shift Analysis Sam Honours/SEA Activity temporal shift by community [Multinomial] Sam Honours")
```

###Load relevant packages

```{r}
#Load libraries
library(tidyverse);library(plyr);library(mclogit)

```

###Load and inspect the dataframe

```{r}
#Load dataframe
caps = as.data.frame(read.csv("SEA_Activity_dataset_for_community_guild-level_analyses_20230306.csv"))

#Inspect dataframe
head(caps)
str(caps)
names(caps)
anyNA(caps)
```

#Using Multinomial logistic Regressions Models to predict the probability of the community being nocturnal, diurnal and crepuscular

Unlike binomial logistic regressions, Multinomial logistic regressions can model a categorical response variable with 3 or more outcomes. In our study, the outcomes are detections during the day, twilight and night. We will be using this model to essentially predict the probabilities of wildlife community being diurnal, crepuscular and nocturnal over a gradient of forest integrity (independent/predictor variable).

But first, let's create a "landscape" column to represent our 10 landscapes across Southeast Asia. This will be treated as our random effect in our study. 

```{r}
#Check how many surveys we have!
sort(unique(caps$survey_id))

#Let's create our "landscape" column
caps$landscape = NA
caps$landscape[caps$survey_id == "BBS"] = "Bukit_Barisan_Selatan"
caps$landscape[caps$survey_id %in% c("Danum_Valley_2019a", "Danum2018")] = "Danum_Valley"
caps$landscape[caps$survey_id == "Kerinci"] = "Kerinci_Seblat"
caps$landscape[caps$survey_id == "KhaoChong2018"] = "Khao_Chong"
caps$landscape[caps$survey_id == "KhaoYai2019"] = "Khao_Yai"
caps$landscape[caps$survey_id == "Lambir2017"] = "Lambir_hills"
caps$landscape[caps$survey_id == "Leuser"] = "Gunung_Leuser"
caps$landscape[caps$survey_id %in% c("Pasoh_TEAM_2013", "Pasoh_TEAM_2014", "Pasoh_TEAM_2015", "Pasoh_TEAM_2017")] = "Pasoh"
caps$landscape[caps$survey_id == "Singapore"] = "Singapore"
caps$landscape[caps$survey_id %in% c("Ulu_Muda_2015a", "Ulu_Muda_2015b", "Ulu_Muda_2015c", "Ulu_Muda_2015d", "Ulu_Muda_2016a", "Ulu_Muda_2016b", "Ulu_Muda_2016c")] = "Ulu_Muda"

#Check whether there is 10 landscapes
sort(unique(caps$landscape))

#Check whether there is "NA"
anyNA(caps$landscape) #good

#set the landscape column as a factor
caps$landscape = as.factor(caps$landscape)
str(caps) #great!

```

###Defining the temporal categorisations ("outcomes"): Day, Twilight and Dusk

Here, we will be defining our detections based on the time of day. We define "day" detections as being detections found between 0731 hr to 1629 hr, "twilight" detections as being detections found between 0430 hr to 0730 hr or 1630 hr to 1930 hr, and lastly "night" detections as being detections found between 1931hr to 0429 hr.

```{r}
#Include the respective time of day categories
caps$time_of_day = "NA"
caps$time_of_day[caps$time.rad >= 5.109451 | caps$time.rad <= 1.178096] = "night"
caps$time_of_day[caps$time.rad >= 1.967859 & caps$time.rad <= 4.319689] = "day"
caps$time_of_day[caps$time_of_day == "NA"] = "twilight"
sort(unique(caps$time_of_day))

#Set the time of day column as a factor
caps$time_of_day = as.factor(caps$time_of_day)
str(caps) #great!

#"Twilight" category as baseline category
caps$time_of_day <- relevel(caps$time_of_day, ref = "twilight")
```

###Building the Multinomial Logistic Regression Model with one random effect using "mclogit" package

This package allows for the inclusion of random effects. For these models, we will use one random effects, namely landscape.

Guidelines for building the multinomial equation using mblogit():

*random = random effects (~1 | Your random effect)

*estimator = Maximum Likelihood (ML) or Restricted Maximum Likelihood (REML)

*dispersion = "Afroz", "Fletcher", "Pearson"

*method = "PQL" or "MQL"

We will use body size and feeding guild as the other fixed effects here as well.  

```{r}
#Select only pigs and macaques and build model
caps = caps %>% filter(Species %in% c("Sus_barbatus", "Sus_scrofa", "Macaca_nemestrina", "Macaca_fascicularis"))

#Reverse forest integrity
caps$disturbance_index <- max(caps$forest_integrity) - caps$forest_integrity

#Build the model
model = mblogit(time_of_day ~ disturbance_index, random = ~1 | landscape, data = caps, estimator = "REML", dispersion = "Afroz", method = "PQL")

```

###Extract the p-value and estimates for best model and store it in a dataframe

```{r}
#Obtain the summary statistics of the model
sum = summary(model)
sum = as.data.frame(sum$coefficients)

#Clean it up
sum = sum %>% 
  mutate(formula = row.names(sum)) %>% 
  dplyr::rename("estimates" = "Estimate", "p-value" = "Pr(>|z|)")

#save it
write.csv(sum, "P&M_sumstats_[landscape_only]_20231005.csv", row.names = F)

#keep environment clean!
rm(sum)
```

###Visualizing the effect of forest integrity on the probability of being active at day, twilight, and night

#####Calculate the predicted probabilities 

```{r}
#Create new dataframe to store predicted probabilities
newdata <-  expand.grid(
disturbance_index = seq(min(caps$disturbance_index), max(caps$disturbance_index), length.out = 100),
landscape = unique(caps$landscape))

#Calculate predicted probabilities
predictions <- predict(model, newdata = newdata, type = "response")
newdata = cbind(newdata, predictions)

#Keep environment clean!
rm(predictions)

```

######Loop to compute smoothed/average values

```{r}
# Create a dataframe to be filled with smoothed/averaged values
smoothed_vals <- data.frame(disturbance_index = unique(newdata$disturbance_index))

k = "day"; l = "Pasoh" 

for (k in c("day", "twilight", "night")) {#repeat for each diel category
    
    #Set the columns of each diel category to zero
    smoothed_vals[[k]] <- 0 
    
    for (l in unique(newdata$landscape)) {#repeat for each unique landscape
      
      #fit a smooth curve to predict a specific time of day (like "day" or "night") based on disturbance index
      loess_fit <- loess(formula = as.formula(paste(k, "~ disturbance_index")), data = subset(newdata, landscape == l))
      
      #get the expected values (predictions) for a specific time of day (like "day") based on various disturbance index values and adding them up for each landscape. Later, this accumulated total will be averaged out to show the general trend  
      smoothed_vals[[k]] <- smoothed_vals[[k]] + predict(loess_fit, newdata = data.frame(disturbance_index = smoothed_vals$disturbance_index))
      
    }
    
    #Average the smoothed values
    smoothed_vals[[k]] <- smoothed_vals[[k]] / length(unique(newdata$landscape))
  }

#Keep environment clean!
rm(loess_fit,k,l)

```

#####Loop to compute standard errors for smoothed/average values

```{r}
# Create a dataframe to be filled with standard errors
se_data <- data.frame(disturbance_index = unique(newdata$disturbance_index))

k = "day"; l = 0.00; u = "Pasoh"

for (k in c("day", "twilight", "night")) {#repeat for each diel category
    
    #Set the columns of each diel category to zero to store standard errors
    se_data[[paste(k, "se", sep = "_")]] <- NA
    
    for (l in unique(newdata$disturbance_index)) {#repeat for each unique value of forest integrity
      
      #creates a list of zeros, one for each unique landscape, to store the predicted values (from the smooth curve) for that particular disturbance index and time of day.
        smoothed_values_for_fi <- numeric(length = length(unique(newdata$landscape)))
        
        for (u in seq_along(unique(newdata$landscape))) {#repeat for each landscape
          
          #fits a smooth curve, using only data from the current landscape
          loess_fit <- loess(formula = as.formula(paste(k, "~ disturbance_index")), data = subset(newdata, landscape == unique(newdata$landscape)[u]))
          
          #predicts the time of day (like "day") for the current disturbance index value using the smooth curve from the previous step. It stores this predicted value in our list of zeros from earlier.
            smoothed_values_for_fi[u] <- predict(loess_fit, newdata = data.frame(disturbance_index = l))
          
        }
      #calculate the standard error of each prediction for each landsape at each unique forest integrity value for each diel category
        se_data[se_data$disturbance_index == l, paste(k, "se", sep = "_")] <- sd(smoothed_values_for_fi) / sqrt(length(smoothed_values_for_fi))

    }
  }

#keep environment clean
rm(k,l,u,smoothed_values_for_fi,loess_fit)
```

#####Merging the smoothed values and standard errors

```{r}
#Merge them
df = merge(smoothed_vals, se_data, by = "disturbance_index")
rm(smoothed_vals,se_data)
```

#####Visualise the best model

```{r}
plot <- ggplot(df, aes(x = disturbance_index)) + 
    
    #plot the "day" category
    geom_line(aes(y = day, color = "Day"), linewidth = 1.5, linetype = "dashed") + 
    geom_ribbon(aes(ymin = day - 1.96 * day_se, ymax = day + 1.96 * day_se, fill = "Day"), alpha = 0.2) +
    
    #plot the "twilight" category
    #geom_line(aes(y = twilight, color = "Twilight"), linetype = linetype_twilight, linewidth = 1.5) +
    geom_ribbon(aes(ymin = twilight - 1.96 * twilight_se, ymax = twilight + 1.96 * twilight_se, fill = "Twilight"), alpha = 0.2) +
    
    #plot the "night" category
    geom_line(aes(y = night, color = "Night"), linetype = "dashed", linewidth = 1.5) +
    geom_ribbon(aes(ymin = night - 1.96 * night_se, ymax = night + 1.96 * night_se, fill = "Night"), alpha = 0.2) +
      
    #Choose appropriate colours
    scale_color_manual(values = c("Day" = "orange", "Twilight" = "purple", "Night" = "darkblue")) +
    scale_fill_manual(values = c("Day" = "orange", "Twilight" = "purple", "Night" = "darkblue")) +
      
    #fixed the range of the y-axis
    coord_cartesian(ylim = c(0.0, 1.0)) +  
    
    #Add axis titles
    labs(x = "Disturbance Index", y = "Mean Predicted Probability") +
    theme_classic()

plot

#save it
ggsave("P&M_plot_[landscape_only]_20231003.PDF", plot = plot, height = 6, width = 8, units = "in")
rm(df,model,newdata,plot)

```

#Building model using two random effects (i.e., species and landscape)

###Building the Multinomial Logistic Regression Model with two random effects using "mclogit" package

This package allows for the inclusion of random effects. For these models, we will use two random effects, namely landscape and species.

Guidelines for building the multinomial equation using mblogit():

*random = random effects (~1 | Your random effect)

*estimator = Maximum Likelihood (ML) or Restricted Maximum Likelihood (REML)

*dispersion = "Afroz", "Fletcher", "Pearson"

*method = "PQL" or "MQL"

We will use body size and feeding guild as the other fixed effects here as well.  

```{r}
model = mblogit(time_of_day ~ disturbance_index, random = c(~1 | landscape, ~1 | Species), data = caps, estimator = "REML", dispersion = "Afroz", method = "PQL")
```

###Extract the p-value and estimates for best model and store it in a dataframe

```{r}
#Obtain the summary statistics of the model
sum = summary(model)
sum = as.data.frame(sum$coefficients)

#Clean it up
sum = sum %>% 
  mutate(formula = row.names(sum)) %>% 
  dplyr::rename("estimates" = "Estimate", "p-value" = "Pr(>|z|)")

#save it
write.csv(sum, "P&M_sumstats_[landscape_species]_20231005.csv", row.names = F)

#keep environment clean!
rm(sum)
```

###Visualizing the model

#####Calculate the predicted probabilities 

```{r}
#Create new dataframe to store predicted probabilities
newdata <-  expand.grid(
disturbance_index = seq(min(caps$disturbance_index), max(caps$disturbance_index), length.out = 100),
landscape = unique(caps$landscape),
Species = unique(caps$Species))

#Calculate predicted probabilities
predictions <- predict(model, newdata = newdata, type = "response")
newdata = cbind(newdata, predictions)

#Keep environment clean!
rm(predictions)
```

######Loop to compute smoothed/average values

```{r}
# Create a dataframe to be filled with smoothed/averaged values
smoothed_vals <- data.frame(disturbance_index = unique(newdata$disturbance_index))

k = "day"; l = "Pasoh"; s = "Macaca_nemestrina"

for (k in c("day", "twilight", "night")) { # Repeat for each diel category

  # Set the columns of each diel category to zero
  smoothed_vals[[k]] <- 0

  for (l in unique(newdata$landscape)) { # Repeat for each unique landscape
    
    for (s in unique(newdata$Species)) { # Repeat for each unique Species
      
      # Subset the data for the current landscape and Species
      subset_data <- subset(newdata, landscape == l & Species == s)

      # Fit a smooth curve to predict a specific time of day (like "day" or "night") based on disturbance index
      loess_fit <- loess(formula = as.formula(paste(k, "~ disturbance_index")), data = subset_data)

      # Get the expected values (predictions) for a specific time of day (like "day") based on various disturbance index values
      smoothed_vals[[k]] <- smoothed_vals[[k]] + predict(loess_fit, newdata = data.frame(disturbance_index = smoothed_vals$disturbance_index))
    }
    
    #Average the smoothed values for the current landscape
    smoothed_vals[[k]] <- smoothed_vals[[k]] / length(unique(newdata$Species))
  }
}

#Keep environment clean!
rm(loess_fit, subset_data,k,l,s)
```

#####Loop to compute standard errors for smoothed/average values

```{r}
# Create a dataframe to be filled with standard errors
se_data <- data.frame(disturbance_index = unique(newdata$disturbance_index))

k = "day"; l = 0; u = 1; s = 1

for (k in c("day", "twilight", "night")) { # Repeat for each diel category

  # Set the columns of each diel category to zero to store standard errors
  se_data[[paste(k, "se", sep = "_")]] <- NA

  for (l in unique(newdata$disturbance_index)) { # Repeat for each unique value of forest integrity
    
    # Create a list to store smoothed values for the current disturbance index value and diel category
    smoothed_values_for_fi <- numeric(length = length(unique(newdata$landscape) * unique(newdata$Species)))

    for (u in seq_along(unique(newdata$landscape))) { # Repeat for each landscape

      for (s in seq_along(unique(newdata$Species))) { # Repeat for each Species
        
        # Subset the data for the current landscape and Species
        subset_data <- subset(newdata, landscape == unique(newdata$landscape)[u] & Species == unique(newdata$Species)[s])
        
        # Fit a smooth curve using only data from the current landscape and Species
        loess_fit <- loess(formula = as.formula(paste(k, "~ disturbance_index")), data = subset_data)

        # Predict the time of day (like "day") for the current disturbance index value using the smooth curve
        smoothed_values_for_fi[u + (s - 1) * length(unique(newdata$landscape))] <- predict(loess_fit, newdata = data.frame(disturbance_index = l))
      }
    }
    
    # Calculate the standard error of each prediction for each landscape at each unique forest integrity value for each diel category
    se_data[se_data$disturbance_index == l, paste(k, "se", sep = "_")] <- sd(smoothed_values_for_fi) / sqrt(length(smoothed_values_for_fi))

  }
}

#keep environment clean
rm(k,l,u,s,smoothed_values_for_fi,loess_fit,subset_data)
```

#####Merging the smoothed values and standard errors

```{r}
#Merge them
df = merge(smoothed_vals, se_data, by = "disturbance_index")
rm(smoothed_vals,se_data)
```

#####Visualise the model

```{r}
plot <- ggplot(df, aes(x = disturbance_index)) + 
    
    #plot the "day" category
    geom_line(aes(y = day, color = "Day"), linewidth = 1.5, linetype = "dashed") + 
    geom_ribbon(aes(ymin = day - 1.96 * day_se, ymax = day + 1.96 * day_se, fill = "Day"), alpha = 0.2) +
    
    #plot the "twilight" category
    #geom_line(aes(y = twilight, color = "Twilight"), linetype = linetype_twilight, linewidth = 1.5) +
    geom_ribbon(aes(ymin = twilight - 1.96 * twilight_se, ymax = twilight + 1.96 * twilight_se, fill = "Twilight"), alpha = 0.2) +
    
    #plot the "night" category
    geom_line(aes(y = night, color = "Night"), linetype = "dashed", linewidth = 1.5) +
    geom_ribbon(aes(ymin = night - 1.96 * night_se, ymax = night + 1.96 * night_se, fill = "Night"), alpha = 0.2) +
      
    #Choose appropriate colours
    scale_color_manual(values = c("Day" = "orange", "Twilight" = "purple", "Night" = "darkblue")) +
    scale_fill_manual(values = c("Day" = "orange", "Twilight" = "purple", "Night" = "darkblue")) +
      
    #fixed the range of the y-axis
    coord_cartesian(ylim = c(0.0, 1.0)) +  
      
    #Add axis titles
    labs(x = "Disturbance Index", y = "Mean Predicted Probability") +
    theme_classic()

plot

#save it
ggsave("P&M_plot_[landscape_species]_20231002.PDF", plot = plot, height = 6, width = 8, units = "in")

```
